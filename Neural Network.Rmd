---
title: "Neural Network"
author: "Arnab Aich"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r setup, include=FALSE}

library(tensorflow)
library(torchvision)
library(torch)
library(ggplot2)
# install_tensorflow()
# tf$device("gpu")
# tf$config$list_physical_devices("GPU")
library(keras)
library(png)
library(colorspace)
library(imager)
library(SpatialPack)
library(dplyr)
library(tidyverse)
library(Thermimage)
```

# Data import and set up

```{r,tidy=TRUE, tidy.opts=list(width.cutoff=60)}

# import .png as a dataset
img <- readPNG("D:/OneDrive - Florida State University/MyFSU_OneDrive/FSU Course Work/5635/Datasets/horse001b.png")
# arrange pixel values for proper printing
img_gray <- mirror.matrix(t(as.matrix(RGB2gray(img))))
# Load pixel values
train_labels <- as.vector(as.matrix(RGB2gray(img)))
# The image
image(img_gray,axes = FALSE,col = grey(seq(0, 1, length = 255)))
# sorting co ordinates
X = cbind(rows = scale(rep(seq(1,84),128)),cols = scale(as.vector(sapply(seq(1,84),function(x) rep(x,128)))))
# Load pixel co-ordinates
train_data <-X 


```

# One hidden layer 128

```{r}

# Define the model architecture
model <- keras_model_sequential()
model%>%
# Input layer
  
layer_dense(units = length(train_labels), activation = "relu", input_shape = dim(train_data)[2]) %>%
# one hidden layer
  layer_dense(units = 128, activation = "relu",use_bias = FALSE,batch_size = 64) %>%
# output layers  
  layer_dense(units = 1,use_bias = FALSE)

# Compile the model
model %>% compile(
  loss = "mean_squared_error",

 optimizer = optimizer_sgd(learning_rate = learning_rate_schedule_inverse_time_decay(initial_learning_rate = 0.01, decay_steps = 100,decay_rate = 0.5))
)

# Train the model
history <- model %>% fit(
  train_data, train_labels,
  epochs = 300
)

```

```{r}
ggplot()+geom_line(aes(x=seq(1,history$params$epochs),y=history$metrics$loss),color = 'blue',linetype=1,size=1.2)+xlab('Epoch')+ylab('Loss')
pred = predict(model,train_data);dim(pred)
output = matrix(pred,ncol= 128)
image(output,axes = FALSE,col = grey(seq(0, 1, length = 255)))
```

```{r include=FALSE}
rm(model)
rm(history)
rm(output)
```

# Two hidden layers (32, 128)

```{r message=FALSE, warning=FALSE, include=FALSE}

# Define the model architecture
model <- keras_model_sequential()
model %>%
# input layer
 layer_dense(units = length(train_labels), activation = "relu", input_shape = dim(train_data)[2]) %>%
# two hidden layers
  layer_dense(units = 32, activation = "relu",use_bias = FALSE,batch_size = 64) %>%
  layer_dense(units = 128, activation = "relu",use_bias = FALSE,batch_size = 64) %>%
# output layers  
  layer_dense(units = 1,use_bias = FALSE)

# Compile the model
model %>% compile(
  loss = "mean_squared_error",

 optimizer = optimizer_sgd(learning_rate = learning_rate_schedule_inverse_time_decay(initial_learning_rate = 0.4, decay_steps = 100,decay_rate = 0.5))
)

# Train the model
history <- model %>% fit(
  train_data, train_labels,
  epochs = 300
)
```

```{r}

ggplot()+geom_line(aes(x=seq(1,history$params$epochs),y=history$metrics$loss),color = 'blue',linetype=1,size=1.2)+xlab('Epoch')+ylab('Loss')
# output image
output = mirror.matrix(t(matrix(predict(model,train_data),ncol= 128)))
image(output,axes = FALSE,col = grey(seq(0, 1, length = 255)))


```

```{r include=FALSE}
rm(model)
rm(history)
rm(output)
```

# Three hidden layers (32 ,64 ,128)

```{r,tidy=TRUE, tidy.opts=list(width.cutoff=60)}

# Define the model architecture
model <- keras_model_sequential()
model %>%
  # input layers
  layer_dense(units = length(train_labels), activation = "relu", input_shape = dim(train_data)[2]) %>%
  # Three hidden layers
  layer_dense(units = 32, activation = "relu",use_bias = FALSE,batch_size = 64) %>%
  layer_dense(units = 64, activation = "relu",use_bias = FALSE,batch_size = 64) %>%
  layer_dense(units = 128, activation = "relu",use_bias = FALSE,batch_size = 64) %>%
# output layers  
  layer_dense(units = 1,use_bias = FALSE)

# Compile the model
model %>% compile(
  loss = "mean_squared_error",

 optimizer = optimizer_sgd(learning_rate = learning_rate_schedule_inverse_time_decay(initial_learning_rate = 0.4, decay_steps = 100,decay_rate = 0.5,staircase = TRUE))
)

# Train the model
history <- model %>% fit(
  train_data, train_labels,
  epochs = 300
)

```

```{r,tidy=TRUE, tidy.opts=list(width.cutoff=60)}
ggplot()+geom_line(aes(x=seq(1,history$params$epochs),y=history$metrics$loss),color = 'blue',linetype=1,size=1.2)+xlab('Epoch')+ylab('Loss')

# output image

output = mirror.matrix(t(matrix(predict(model,train_data),ncol= 128)))
image(output,axes = FALSE,col = grey(seq(0, 1, length = 255)))
```

```{r include=FALSE}
rm(model)
rm(history)
rm(output)
```

# Four hidden Layers (32,64,128,128)

```{r echo=TRUE, message=FALSE, warning=FALSE,tidy=TRUE, tidy.opts=list(width.cutoff=60)}
# Define the model architecture
model <- keras_model_sequential()
model %>%
# input layer
  layer_dense(units = length(train_labels),use_bias = FALSE, input_shape = dim(train_data)[2]) %>%
# four hidden layers  
  layer_dense(units = 32, activation = "relu",use_bias = FALSE,batch_size = 64,) %>%
  layer_dense(units = 64, activation = "relu",use_bias = FALSE,batch_size = 64) %>%
  layer_dense(units = 128, activation = "relu",use_bias = FALSE,batch_size = 64) %>%
  layer_dense(units = 128, activation = "relu",use_bias = FALSE,batch_size = 64) %>%
# output layers  
  layer_dense(units = 1,use_bias = FALSE)

# Compile the model
model %>% compile(
  loss = "mean_squared_error",

 optimizer = optimizer_sgd(learning_rate = learning_rate_schedule_inverse_time_decay(initial_learning_rate = 0.4, decay_steps = 100,decay_rate = 0.5))
)

# Train the model
history <- model %>% fit(
  train_data, train_labels,
  epochs = 300
)
```

```{r,tidy=TRUE, tidy.opts=list(width.cutoff=60)}
ggplot()+geom_line(aes(x=seq(1,history$params$epochs),y=history$metrics$loss),color = 'blue',linetype=1,size=1.2)+xlab('Epoch')+ylab('Loss')
# output image
output = mirror.matrix(t(matrix(predict(model,train_data),ncol= 128)))
image(output,axes = FALSE,col = grey(seq(0, 1, length = 255)))
```
